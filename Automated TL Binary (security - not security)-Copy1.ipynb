{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partially adopted from Tensorflow/docs/basic_text_classification\n",
    "#https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\n",
    "\n",
    "#As well as https://developers.google.com/machine-learning/guides/text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ParseList import parseList\n",
    "from GetSeeAlso import getSeeAlso\n",
    "from MakeLabels import makeLabels\n",
    "from PopulateList import populateList\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING SENTENCES TO LIBRARY\n",
    "import json\n",
    "\n",
    "#declare testLabels as an empty list\n",
    "testLabels = []\n",
    "\n",
    "#Keeping track of used sentences\n",
    "used = []\n",
    "\n",
    "#Keeping track of used sentences separately\n",
    "security = []\n",
    "nosecurity = []\n",
    "\n",
    "#this is the smallest number of unique non-functional requirements of a single class\n",
    "LIMIT = 270\n",
    "\n",
    "with open(\"Data/Consolidated_data.txt\",\"r\") as f:\n",
    "    \n",
    "    #Go line by line in original data file\n",
    "    for line in f:\n",
    "        #find where class begins\n",
    "        front = line.find(\"\\\"class\\\":\\\"\")\n",
    "        #find where class ends (where sentence begins)\n",
    "        end = line.find(\"\\\",\\\"sentence\\\":\\\"\")\n",
    "        #substring line based on front and end above\n",
    "        reqClass = (line[(front+9):end]).lower()\n",
    "\n",
    "        #sentence\n",
    "        temp = line.find(\"\\\"sentence\\\":\\\"\")\n",
    "        #Cut out the sentence part\n",
    "        sentence = (line[(temp+12):-4]).lower()\n",
    "        #Remove all symbols, numbers, and spaces. Split into list of words\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        #Unicode each word because stemmer likes it that way\n",
    "        sentence = [unicode(i, 'utf-8') for i in sentence]\n",
    "        #Stem each word separately\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        #Check word by word, if the word is not in stop words and not a single letter, join\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1)\n",
    "        \n",
    "        if sentence in used:\n",
    "            continue\n",
    "        else:\n",
    "            if \"security\" in reqClass and (len(security) < LIMIT):\n",
    "                used.append(sentence)\n",
    "                security.append(sentence)\n",
    "                testLabels.append(1)\n",
    "#             if \"security\" not in reqClass and (len(nosecurity) < LIMIT):\n",
    "#                 used.append(sentence)\n",
    "#                 nosecurity.append(sentence)\n",
    "#                 testLabels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "0\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "#Must all be balanced for valid results.\n",
    "#Commented out the total number of sentences in the document.\n",
    "print len(security) #270\n",
    "print len(nosecurity)\n",
    "print len(used) #540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "x_xl1 = []\n",
    "x_xl2 = []\n",
    "with open('Data/NFR.csv') as f:\n",
    "    for sentence in f:\n",
    "        cl = sentence[:sentence.find(',')]\n",
    "        sentence = sentence[sentence.find(',')+1:]\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        sentence = [unicode(i, 'utf-8', errors='ignore') for i in sentence]\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1).strip()\n",
    "        if cl == 'Usability':\n",
    "            x_xl2.append(sentence)\n",
    "        else:\n",
    "            x_xl1.append(sentence)\n",
    "del x_xl1[min(len(x_xl1), len(x_xl2)):]\n",
    "del x_xl2[min(len(x_xl1), len(x_xl2)):]\n",
    "\n",
    "print(len(x_xl1))\n",
    "print(len(x_xl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article names in the privacy list: 300\n",
      "Article names in the not privacy list: 300\n"
     ]
    }
   ],
   "source": [
    "securityList1, notSecurityList = populateList(\"Data/enwiki-latest-all-titles-in-ns0\", \"privacy\", 300, True)\n",
    "# seeAlso = getSeeAlso(*securityList)\n",
    "# securityList += seeAlso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "security = []\n",
    "with open('Data/negative10k_usability.txt') as f:\n",
    "    for sentence in f:\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        sentence = [unicode(i, 'utf-8', errors='ignore') for i in sentence]\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1).strip()\n",
    "        security.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels made successfully\n",
      "Labels made successfully\n"
     ]
    }
   ],
   "source": [
    "y_xl = makeLabels(x_xl1, 0)\n",
    "y_xl += makeLabels(x_xl2, 1)\n",
    "x_xl = x_xl1 + x_xl2\n",
    "x_xl, y_xl = shuffle(x_xl, y_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Security length:  9769\n",
      "Not Security length:  14225\n",
      "Labels made successfully\n",
      "Labels made successfully\n",
      "\n",
      "Test set length:  270\n",
      "Test labels set length:  270\n",
      "\n",
      "Validation set length:  976\n",
      "Validation labels set length:  976\n",
      "\n",
      "Train set length:  18562\n",
      "Train labels set length:  18562\n"
     ]
    }
   ],
   "source": [
    "#Parse two wikipedia lists\n",
    "#security = parseList(*securityList)\n",
    "notSecurity = parseList(*notSecurityList)\n",
    "print \"\\nSecurity length: \", len(security)\n",
    "print \"Not Security length: \", len(notSecurity)\n",
    "\n",
    "#Make entries be same length, pick minimum of two lengths\n",
    "deleteAfter = min(len(security), len(notSecurity))\n",
    "\n",
    "#If min length is less than 100 sentences, test is invalid\n",
    "if deleteAfter < 100:\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    del security[deleteAfter:]\n",
    "    del notSecurity[deleteAfter:]\n",
    "\n",
    "if(len(security) != len(notSecurity) and (len(security) != len(set(security)) or len(notSecurity) != len(set(notSecurity)))):\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    #Generate labels\n",
    "    trainLabels = makeLabels(security, 1)\n",
    "    trainLabels += makeLabels(notSecurity, 0)\n",
    "\n",
    "    #Collapse into single list\n",
    "    trainData = security + notSecurity\n",
    "\n",
    "    #Shuffle two lists, save the order\n",
    "    trainData, trainLabels = shuffle(trainData, trainLabels)\n",
    "    #used, testLabels = shuffle(used, testLabels)\n",
    "\n",
    "    #Get validation data\n",
    "    validationToTrainRatio = 0.05\n",
    "    validationSize = int(validationToTrainRatio * len(trainData))\n",
    "    validationData = trainData[:validationSize]\n",
    "    validationLabels = trainLabels[:validationSize]\n",
    "    trainData = trainData[validationSize:]\n",
    "    trainLabels = trainLabels[validationSize:]\n",
    "\n",
    "    \n",
    "    #Sanity check\n",
    "    print \"\\nTest set length: \", len(used)\n",
    "    print \"Test labels set length: \", len(testLabels)\n",
    "    print \"\\nValidation set length: \", len(validationData)\n",
    "    print \"Validation labels set length: \", len(validationLabels)\n",
    "    print \"\\nTrain set length: \", len(trainData)\n",
    "    print \"Train labels set length: \", len(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts, test_texts):\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1,1),  # Use 1-grams + 2-grams + 3-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'min_df': 2, #Words that appear less than this value do not contribute\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    # Vectorize test texts.\n",
    "    x_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(20000, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldeng/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:1569: UserWarning: Only (<type 'numpy.float64'>, <type 'numpy.float32'>, <type 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "trainData, valData, testData = ngram_vectorize(trainData, trainLabels, validationData, x_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the model method\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(keras.layers.Dense(units=units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=32, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(keras.layers.Dense(units=num_classes, activation=tf.nn.sigmoid))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 12204)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                781120    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 783,233\n",
      "Trainable params: 783,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#For parameters refer to the upper cell\n",
    "model = mlp_model(2, 64, 0.3, trainData.shape[1:], 1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate could be further decreased for additional accuracy\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18562 samples, validate on 976 samples\n",
      "Epoch 1/20\n",
      " - 2s - loss: 0.6680 - acc: 0.7800 - val_loss: 0.6056 - val_acc: 0.9682\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.5327 - acc: 0.9484 - val_loss: 0.4041 - val_acc: 0.9744\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.3302 - acc: 0.9570 - val_loss: 0.2061 - val_acc: 0.9723\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.1843 - acc: 0.9615 - val_loss: 0.1118 - val_acc: 0.9723\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.1221 - acc: 0.9637 - val_loss: 0.0792 - val_acc: 0.9734\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.0951 - acc: 0.9689 - val_loss: 0.0678 - val_acc: 0.9754\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.0825 - acc: 0.9706 - val_loss: 0.0637 - val_acc: 0.9754\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.0722 - acc: 0.9722 - val_loss: 0.0616 - val_acc: 0.9744\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.0670 - acc: 0.9739 - val_loss: 0.0608 - val_acc: 0.9734\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.0639 - acc: 0.9755 - val_loss: 0.0610 - val_acc: 0.9764\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.0578 - acc: 0.9783 - val_loss: 0.0602 - val_acc: 0.9754\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.0541 - acc: 0.9783 - val_loss: 0.0612 - val_acc: 0.9723\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.0572 - acc: 0.9780 - val_loss: 0.0630 - val_acc: 0.9734\n"
     ]
    }
   ],
   "source": [
    "#callbacks will prevent model from running if val_loss starts to increase\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = model.fit(trainData,\n",
    "                    trainLabels,\n",
    "                    epochs=20,\n",
    "                    callbacks = callbacks,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(valData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(testData)\n",
    "pred_labels = []\n",
    "occurences = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "for x in results:\n",
    "    if x <= 0.05:\n",
    "        occurences[0] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.15 and x > 0.05:\n",
    "        occurences[1] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.25 and x > 0.15:\n",
    "        occurences[2] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.35 and x > 0.25:\n",
    "        occurences[3] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.45 and x > 0.35:\n",
    "        occurences[4] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.55 and x > 0.45:\n",
    "        occurences[5] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.65 and x > 0.55:\n",
    "        occurences[6] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.75 and x > 0.65:\n",
    "        occurences[7] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.85 and x > 0.75:\n",
    "        occurences[8] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.95 and x > 0.85:\n",
    "        occurences[9] += 1\n",
    "        pred_labels.append(1)\n",
    "    elif x <= 0.98 and x > 0.95:\n",
    "        occurences[10] += 1\n",
    "        pred_labels.append(1)\n",
    "    elif x > 0.98:\n",
    "        occurences[11] += 1\n",
    "        pred_labels.append(1)\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for x, y in zip(pred_labels, y_xl):\n",
    "    if x == 1 and y == 1:\n",
    "        TP += 1\n",
    "    elif x == 1 and y == 0:\n",
    "        FP += 1\n",
    "    elif x == 0 and y == 1:\n",
    "        FN += 1\n",
    "    elif x == 0 and y == 0:\n",
    "        TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142, 62, 20, 13, 16, 5, 13, 10, 7, 11, 8, 7]\n",
      "Accuracy: 0.5064\n",
      "\n",
      "Precision: 0.9298\n",
      "Recall: 0.5265\n",
      "F-score: 0.6723\n",
      "\n",
      "TP 14, TN 145, FP 12, FN 143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[145,  12],\n",
       "       [143,  14]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print occurences\n",
    "accuracy = (TP+TN)/float(TP+TN+FP+FN)\n",
    "precision = (TP+TN)/float(TP+TN+FP)\n",
    "recall = (TP+TN)/float(TP+TN+FN)\n",
    "fscore = (2*precision*recall)/float(precision+recall)\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"\\nPrecision: %.4f\\nRecall: %.4f\\nF-score: %.4f\" % (precision, recall, fscore))\n",
    "print(\"\\nTP %d, TN %d, FP %d, FN %d\" % (TP, TN, FP, FN))\n",
    "\n",
    "df = pd.DataFrame({'Occurences': occurences}, index=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98])\n",
    "ax = df.plot.bar(rot=0)\n",
    "\n",
    "confusion_matrix(y_xl, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5064\n",
      "\n",
      "Precision: 0.5385\n",
      "Recall: 0.0892\n",
      "F-score: 0.1530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[145,  12],\n",
       "       [143,  14]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(y_xl, pred_labels)\n",
    "precision = precision_score(y_xl, pred_labels)\n",
    "recall = recall_score(y_xl, pred_labels)\n",
    "fscore = f1_score(y_xl, pred_labels)\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"\\nPrecision: %.4f\\nRecall: %.4f\\nF-score: %.4f\" % (precision, recall, fscore))\n",
    "\n",
    "confusion_matrix(y_xl, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
