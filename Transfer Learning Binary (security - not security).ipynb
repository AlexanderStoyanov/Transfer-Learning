{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partially adopted from Tensorflow/docs/basic_text_classification\n",
    "#https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\n",
    "\n",
    "#As well as https://developers.google.com/machine-learning/guides/text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING SENTENCES TO LIBRARY\n",
    "import json\n",
    "\n",
    "#declare testLabels as an empty list\n",
    "testLabels = []\n",
    "\n",
    "#Keeping track of used sentences\n",
    "used = []\n",
    "\n",
    "#Keeping track of used sentences separately\n",
    "security = []\n",
    "nosecurity = []\n",
    "\n",
    "#this is the smallest number of unique non-functional requirements of a single class\n",
    "LIMIT = 270\n",
    "\n",
    "with open(\"Consolidated_data.txt\",\"r\") as f:\n",
    "    \n",
    "    #Go line by line in original data file\n",
    "    for line in f:\n",
    "        #find where class begins\n",
    "        front = line.find(\"\\\"class\\\":\\\"\")\n",
    "        #find where class ends (where sentence begins)\n",
    "        end = line.find(\"\\\",\\\"sentence\\\":\\\"\")\n",
    "        #substring line based on front and end above\n",
    "        reqClass = (line[(front+9):end]).lower()\n",
    "\n",
    "        #sentence\n",
    "        temp = line.find(\"\\\"sentence\\\":\\\"\")\n",
    "        #Cut out the sentence part\n",
    "        sentence = (line[(temp+12):-4]).lower()\n",
    "        #Remove all symbols, numbers, and spaces. Split into list of words\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        #Unicode each word because stemmer likes it that way\n",
    "        sentence = [unicode(i, 'utf-8') for i in sentence]\n",
    "        #Stem each word separately\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        #Check word by word, if the word is not in stop words and not a single letter, join\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1)\n",
    "        \n",
    "        if sentence in used:\n",
    "            continue\n",
    "        else:\n",
    "            if \"security\" in reqClass and (len(security) < LIMIT):\n",
    "                used.append(sentence)\n",
    "                security.append(sentence)\n",
    "                testLabels.append(1)\n",
    "            elif \"security\" not in reqClass and (len(nosecurity) < LIMIT):\n",
    "                used.append(sentence)\n",
    "                nosecurity.append(sentence)\n",
    "                testLabels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "270\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "#Must all be balanced for valid results.\n",
    "#Commented out the total number of sentences in the document.\n",
    "print len(security) #270\n",
    "print len(nosecurity)\n",
    "print len(used) #540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "def parseList(*titleList):\n",
    "    \n",
    "    train = []\n",
    "    \n",
    "    for title in titleList:\n",
    "        PARAMS = {\n",
    "        'action': \"parse\",\n",
    "        'page': title,\n",
    "        'prop': 'wikitext',\n",
    "        'format': \"json\"\n",
    "        }\n",
    "        \n",
    "        entries = []\n",
    "        pageContent = []\n",
    "        \n",
    "        res = S.get(url=URL, params=PARAMS)\n",
    "        data = res.json()\n",
    "        wikitext = data['parse']['wikitext']['*']\n",
    "        lines = wikitext.split('|-')\n",
    "        b = \",-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()\\'/|=\"\n",
    "        for char in b:\n",
    "            lines[0] = lines[0].replace(char, \"\")\n",
    "\n",
    "        while lines[0].find(\"\\n\") is not (None or -1):\n",
    "            current = lines[0].find(\"\\n\")\n",
    "            entries.append(lines[0][:current+1])\n",
    "            lines[0] = lines[0][current+1:]\n",
    "\n",
    "        for line in entries:\n",
    "            index = entries.index(line)\n",
    "\n",
    "            #removing words that do not carry any meaning (longer than 16 chars)\n",
    "            for key in line.split(\" \"):\n",
    "                if len(key) > 16:\n",
    "                    line = line.replace(key, \"\")\n",
    "\n",
    "            #if line is not empty and wordcount > 20, line will be used for training\n",
    "            if line != '\\n' and (len(line.split(\" \")) > 20):\n",
    "                pageContent.append(line.strip())\n",
    "\n",
    "        for paragraph in pageContent:\n",
    "            for sentence in paragraph.split(\".\"):\n",
    "                sentence = sentence.strip().split(\" \")\n",
    "                if len(sentence) < 10:\n",
    "                    continue\n",
    "                #Stem each word separately\n",
    "                sentence = [stemmer.stem(i) for i in sentence]\n",
    "                #Check word by word, if the word is not in stop words and not a single letter, join\n",
    "                sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1)\n",
    "                train.append(sentence)\n",
    "        del entries[:]\n",
    "        del pageContent[:]\n",
    "        print title, \" parsed successfully\"\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabels(trainData, labelNumber):\n",
    "    #generating labels list that matches pageContent list in length\n",
    "    labels = []\n",
    "    for x in range(0, len(trainData)):\n",
    "        labels.append(labelNumber)\n",
    "    print(\"Labels made successfully\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two lists with names of all Wikipedia pages to be parsed and used for training\n",
    "securityList = [\n",
    "    \"Information security\",\n",
    "    \"Mobile security\",\n",
    "    \"Computer security\",\n",
    "#     \"Computer network\",\n",
    "#     \"Network science\",\n",
    "#     \"Secure by default\",\n",
    "    \"Antivirus software\",\n",
    "    \"Firewall (computing)\",\n",
    "    \"Application security\",\n",
    "#     \"Intrusion detection system\"\n",
    "]\n",
    "\n",
    "notSecurityList = [\n",
    "    \"Medical diagnosis\",\n",
    "    \"Hospital\",\n",
    "    \"Medicine\",\n",
    "    \"Medication\",\n",
    "    \"Database administrator\",\n",
    "    \"Database design\",\n",
    "    \"Database model\",\n",
    "    \"Relational model\",\n",
    "    \"Network administrator\",\n",
    "    \"ISO/IEC 9126\",\n",
    "    \"Email\",\n",
    "    \"Privacy\",\n",
    "    \"Look and feel\",\n",
    "    \"Access control\",\n",
    "    \"Audit\",\n",
    "    \"Service-level agreement\",\n",
    "    \"Availability\",\n",
    "    \"Channel capacity\",\n",
    "    \"End-user license agreement\",\n",
    "    \"Robustness (computer science)\",\n",
    "    \"Software maintenance\",\n",
    "    \"Operability\",\n",
    "    \"Quality (business)\",\n",
    "    \"Usability\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information security  parsed successfully\n",
      "Mobile security  parsed successfully\n",
      "Computer security  parsed successfully\n",
      "Antivirus software  parsed successfully\n",
      "Firewall (computing)  parsed successfully\n",
      "Application security  parsed successfully\n",
      "Medical diagnosis  parsed successfully\n",
      "Hospital  parsed successfully\n",
      "Medicine  parsed successfully\n",
      "Medication  parsed successfully\n",
      "Database administrator  parsed successfully\n",
      "Database design  parsed successfully\n",
      "Database model  parsed successfully\n",
      "Relational model  parsed successfully\n",
      "Network administrator  parsed successfully\n",
      "ISO/IEC 9126  parsed successfully\n",
      "Email  parsed successfully\n",
      "Privacy  parsed successfully\n",
      "Look and feel  parsed successfully\n",
      "Access control  parsed successfully\n",
      "Audit  parsed successfully\n",
      "Service-level agreement  parsed successfully\n",
      "Availability  parsed successfully\n",
      "Channel capacity  parsed successfully\n",
      "End-user license agreement  parsed successfully\n",
      "Robustness (computer science)  parsed successfully\n",
      "Software maintenance  parsed successfully\n",
      "Operability  parsed successfully\n",
      "Quality (business)  parsed successfully\n",
      "Usability  parsed successfully\n",
      "\n",
      "Security length:  1596\n",
      "Not Security length:  2766\n",
      "Labels made successfully\n",
      "Labels made successfully\n",
      "\n",
      "Test set length:  540\n",
      "Test labels set length:  540\n",
      "\n",
      "Validation set length:  638\n",
      "Validation labels set length:  638\n",
      "\n",
      "Train set length:  2554\n",
      "Train labels set length:  2554\n"
     ]
    }
   ],
   "source": [
    "#Parse two wikipedia pages\n",
    "security = parseList(*securityList)\n",
    "notSecurity = parseList(*notSecurityList)\n",
    "print \"\\nSecurity length: \", len(security)\n",
    "print \"Not Security length: \", len(notSecurity)\n",
    "\n",
    "#Make entries be same length, pick minimum of two lengths\n",
    "deleteAfter = min(len(security), len(notSecurity))\n",
    "\n",
    "#If min length is less than 100 sentences, test is invalid\n",
    "if deleteAfter < 100:\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    del security[deleteAfter:]\n",
    "    del notSecurity[deleteAfter:]\n",
    "\n",
    "if(len(security) != len(notSecurity) and (len(security) != len(set(security)) or len(notSecurity) != len(set(notSecurity)))):\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    #Generate labels\n",
    "    trainLabels = makeLabels(security, 1)\n",
    "    trainLabels += makeLabels(notSecurity, 0)\n",
    "\n",
    "    #Collapse into single list\n",
    "    trainData = security + notSecurity\n",
    "\n",
    "    #Shuffle two lists, save the order\n",
    "    trainData, trainLabels = shuffle(trainData, trainLabels)\n",
    "    used, testLabels = shuffle(used, testLabels)\n",
    "\n",
    "    #Get validation data\n",
    "    validationToTrainRatio = 0.2\n",
    "    validationSize = int(validationToTrainRatio * len(trainData))\n",
    "    validationData = trainData[:validationSize]\n",
    "    validationLabels = trainLabels[:validationSize]\n",
    "    trainData = trainData[validationSize:]\n",
    "    trainLabels = trainLabels[validationSize:]\n",
    "\n",
    "    \n",
    "    #Sanity check\n",
    "    print \"\\nTest set length: \", len(used)\n",
    "    print \"Test labels set length: \", len(testLabels)\n",
    "    print \"\\nValidation set length: \", len(validationData)\n",
    "    print \"Validation labels set length: \", len(validationLabels)\n",
    "    print \"\\nTrain set length: \", len(trainData)\n",
    "    print \"Train labels set length: \", len(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts, test_texts):\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1,3),  # Use 1-grams + 2-grams + 3-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'min_df': 2, #Words that appear less than this value do not contribute\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    # Vectorize test texts.\n",
    "    x_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(4000, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.py:1547: UserWarning: Only (<type 'numpy.float64'>, <type 'numpy.float32'>, <type 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "trainData, valData, testData = ngram_vectorize(trainData, trainLabels, validationData, used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the model method\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(keras.layers.Dense(units=units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=num_classes, activation=tf.nn.sigmoid))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                128032    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 128,065\n",
      "Trainable params: 128,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#For parameters refer to the upper cell\n",
    "model = mlp_model(2, 32, 0.3, trainData.shape[1:], 1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate could be further decreased for additional accuracy\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2554 samples, validate on 638 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.6888 - acc: 0.5971 - val_loss: 0.6823 - val_acc: 0.7900\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6716 - acc: 0.8410 - val_loss: 0.6646 - val_acc: 0.8589\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6461 - acc: 0.9037 - val_loss: 0.6406 - val_acc: 0.8824\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6152 - acc: 0.9174 - val_loss: 0.6139 - val_acc: 0.8903\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5811 - acc: 0.9315 - val_loss: 0.5862 - val_acc: 0.8966\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5454 - acc: 0.9389 - val_loss: 0.5581 - val_acc: 0.8981\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5118 - acc: 0.9424 - val_loss: 0.5300 - val_acc: 0.8981\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.4756 - acc: 0.9499 - val_loss: 0.5025 - val_acc: 0.8981\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.4458 - acc: 0.9507 - val_loss: 0.4761 - val_acc: 0.9013\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.4142 - acc: 0.9428 - val_loss: 0.4511 - val_acc: 0.8981\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.3841 - acc: 0.9581 - val_loss: 0.4278 - val_acc: 0.9013\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.3592 - acc: 0.9499 - val_loss: 0.4064 - val_acc: 0.9013\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.3340 - acc: 0.9534 - val_loss: 0.3868 - val_acc: 0.9060\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.3107 - acc: 0.9601 - val_loss: 0.3687 - val_acc: 0.9075\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.2915 - acc: 0.9573 - val_loss: 0.3524 - val_acc: 0.9075\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.2751 - acc: 0.9542 - val_loss: 0.3375 - val_acc: 0.9075\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2595 - acc: 0.9616 - val_loss: 0.3241 - val_acc: 0.9091\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2405 - acc: 0.9620 - val_loss: 0.3120 - val_acc: 0.9122\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.2243 - acc: 0.9648 - val_loss: 0.3013 - val_acc: 0.9138\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.2165 - acc: 0.9612 - val_loss: 0.2917 - val_acc: 0.9138\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.2080 - acc: 0.9648 - val_loss: 0.2829 - val_acc: 0.9185\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.1949 - acc: 0.9699 - val_loss: 0.2750 - val_acc: 0.9185\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1888 - acc: 0.9636 - val_loss: 0.2680 - val_acc: 0.9185\n"
     ]
    }
   ],
   "source": [
    "#callbacks will prevent model from running if val_loss starts to increase\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = model.fit(trainData,\n",
    "                    trainLabels,\n",
    "                    epochs=100,\n",
    "                    callbacks = callbacks,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(valData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalueating model on the testset\n",
    "#[loss, accuracy]\n",
    "print(model.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest of the notebook helps vidualize losses and accuracies by ploting two separate grophs\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf() #clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
