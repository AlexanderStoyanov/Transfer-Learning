{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partially adopted from Tensorflow/docs/basic_text_classification\n",
    "#https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\n",
    "\n",
    "#As well as https://developers.google.com/machine-learning/guides/text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from ParseList import parseList\n",
    "from GetSeeAlso import getSeeAlso\n",
    "from MakeLabels import makeLabels\n",
    "from PopulateList import populateList\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "translate() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-63b3bef8d2c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: translate() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "x_xl = []\n",
    "testLabels = []\n",
    "LIMIT = 150\n",
    "with open('Data/NFR.csv') as f:\n",
    "    for sentence in f:\n",
    "        cl = sentence[:sentence.find(',')]\n",
    "        sentence = sentence[sentence.find(',')+1:]\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        sentence = [unicode(i, 'utf-8', errors='ignore') for i in sentence]\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1).strip()\n",
    "        if cl == 'Security':\n",
    "            x_xl.append(sentence)\n",
    "            testLabels.append(2)\n",
    "        elif cl == 'Usability':\n",
    "            x_xl.append(sentence)\n",
    "            testLabels.append(1)\n",
    "        elif cl == 'Operability':\n",
    "            x_xl.append(sentence)\n",
    "            testLabels.append(0)\n",
    "\n",
    "print(len(x_xl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "securityList = populateList(\"Data/enwiki-latest-all-titles-in-ns0\", \"security\", 400, False)\n",
    "usabilityList = populateList(\"Data/enwiki-latest-all-titles-in-ns0\", \"usability\", 100, False)\n",
    "otherList = populateList(\"Data/enwiki-latest-all-titles-in-ns0\", \"operability\", 250, False)\n",
    "# otherList += getSeeAlso(*otherList)\n",
    "# usabilityList += getSeeAlso(*usabilityList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse two wikipedia pages\n",
    "usability = parseList(*usabilityList)\n",
    "security = parseList(*securityList)\n",
    "other = parseList(*otherList)\n",
    "\n",
    "print \"\\nUsability length: \", len(usability)\n",
    "print \"Security length: \", len(security)\n",
    "print \"Other length: \", len(other)\n",
    "\n",
    "#Make entries be same length, pick minimum of two lengths\n",
    "deleteAfter = min(len(other), len(usability), len(security))\n",
    "\n",
    "#If min length is less than 100 sentences, test is invalid\n",
    "if deleteAfter < 100:\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    del usability[deleteAfter:]\n",
    "    del security[deleteAfter:]\n",
    "    del other[deleteAfter:]\n",
    "\n",
    "l1 = len(other)\n",
    "l2 = len(usability)\n",
    "l3 = len(security)\n",
    "    \n",
    "#Check that all lengths are equal\n",
    "if((l1 != l2 or l2 != l3 or l1 != l3) and (l3 != len(set(security)) or l2 != len(set(usability)) or l1 != len(set(other)))):\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    #Generate labels\n",
    "    trainLabels = makeLabels(usability, 1)\n",
    "    trainLabels += makeLabels(security, 2)\n",
    "    trainLabels += makeLabels(other, 0)\n",
    "\n",
    "    #Collapse into single list\n",
    "    trainData = usability + security + other\n",
    "\n",
    "    #Shuffle two lists, save the order\n",
    "    trainData, trainLabels = shuffle(trainData, trainLabels)\n",
    "    #x_xl, testLabels = shuffle(x_xl, testLabels)\n",
    "    \n",
    "    sm = SMOTE(random_state=42)\n",
    "    trainData, trainLabels = sm.fit_resample(trainData, trainLabels)\n",
    "\n",
    "    #Get validation data\n",
    "    validationToTrainRatio = 0.05\n",
    "    validationSize = int(validationToTrainRatio * len(trainData))\n",
    "    validationData = trainData[:validationSize]\n",
    "    validationLabels = trainLabels[:validationSize]\n",
    "    trainData = trainData[validationSize:]\n",
    "    trainLabels = trainLabels[validationSize:]\n",
    "\n",
    "    \n",
    "    #Sanity check\n",
    "    print \"\\nTest set length: \", len(x_xl)\n",
    "    print \"Test labels set length: \", len(testLabels)\n",
    "    print \"\\nValidation set length: \", len(validationData)\n",
    "    print \"Validation labels set length: \", len(validationLabels)\n",
    "    print \"\\nTrain set length: \", len(trainData)\n",
    "    print \"Train labels set length: \", len(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts, test_texts):\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1,1),  # Use 1-grams + 2-grams + 3-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'min_df': 2, #Words that appear less than this value do not contribute\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    # Vectorize test texts.\n",
    "    x_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(20000, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainData, valData, testData = ngram_vectorize(trainData, trainLabels, validationData, x_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the model method\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(keras.layers.Dense(units=units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=16, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(keras.layers.Dense(units=num_classes, activation=tf.nn.sigmoid))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For parameters refer to the upper cell\n",
    "model = mlp_model(2, 32, 0.3, trainData.shape[1:], 3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate could be further decreased for additional accuracy\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#callbacks will prevent model from running if val_loss starts to increase\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = model.fit(trainData,\n",
    "                    trainLabels,\n",
    "                    epochs=100,\n",
    "                    callbacks = callbacks,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(valData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "results = model.predict(testData)\n",
    "pred_labels = results.argmax(axis=-1)\n",
    "\n",
    "accuracy = accuracy_score(testLabels, pred_labels)\n",
    "precision = precision_score(testLabels, pred_labels, average='weighted')\n",
    "recall = recall_score(testLabels, pred_labels, average='weighted')\n",
    "fscore = f1_score(testLabels, pred_labels, average='weighted')\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"\\nPrecision: %.4f\\nRecall: %.4f\\nF-score: %.4f\" % (precision, recall, fscore))\n",
    "\n",
    "confusion_matrix(testLabels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
