{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partially adopted from Tensorflow/docs/basic_text_classification\n",
    "#https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\n",
    "\n",
    "#As well as https://developers.google.com/machine-learning/guides/text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING SENTENCES TO LIBRARY\n",
    "import json\n",
    "\n",
    "#declare testLabels as an empty list\n",
    "testLabels = []\n",
    "\n",
    "#Keeping track of used sentences\n",
    "used = []\n",
    "\n",
    "#Keeping track of used sentences separately\n",
    "security = []\n",
    "nosecurity = []\n",
    "\n",
    "#this is the smallest number of unique non-functional requirements of a single class\n",
    "LIMIT = 289\n",
    "\n",
    "with open(\"Consolidated_data.txt\",\"r\") as f:\n",
    "    \n",
    "    #Go line by line in original data file\n",
    "    for line in f:\n",
    "        #find where class begins\n",
    "        front = line.find(\"\\\"class\\\":\\\"\")\n",
    "        #find where class ends (where sentence begins)\n",
    "        end = line.find(\"\\\",\\\"sentence\\\":\\\"\")\n",
    "        #substring line based on front and end above\n",
    "        reqClass = (line[(front+9):end]).lower()\n",
    "\n",
    "        #sentence\n",
    "        temp = line.find(\"\\\"sentence\\\":\\\"\")\n",
    "        #Cut out the sentence part\n",
    "        sentence = (line[(temp+12):-4]).lower()\n",
    "        #Remove all symbols, numbers, and spaces. Split into list of words\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        #Unicode each word because stemmer likes it that way\n",
    "        sentence = [unicode(i, 'utf-8') for i in sentence]\n",
    "        #Stem each word separately\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        #Check word by word, if the word is not in stop words and not a single letter, join\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1)\n",
    "        \n",
    "        if sentence in used:\n",
    "            continue\n",
    "        else:\n",
    "            if \"security\" in reqClass and (len(security) < LIMIT):\n",
    "                used.append(sentence)\n",
    "                security.append(sentence)\n",
    "                testLabels.append(1)\n",
    "            elif \"security\" not in reqClass and (len(nosecurity) < LIMIT):\n",
    "                used.append(sentence)\n",
    "                nosecurity.append(sentence)\n",
    "                testLabels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "289\n",
      "578\n"
     ]
    }
   ],
   "source": [
    "#Must all be balanced for valid results.\n",
    "#Commented out the total number of sentences in the document.\n",
    "print len(security) #270\n",
    "print len(nosecurity)\n",
    "print len(used) #540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "def parseList(*titleList):\n",
    "    \n",
    "    train = []\n",
    "    b = \",-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()\\'/|=\\n\"\n",
    "    \n",
    "    for title in titleList:\n",
    "        PARAMS = {\n",
    "        'action': \"parse\",\n",
    "        'page': title,\n",
    "        'prop': 'wikitext',\n",
    "        'format': \"json\"\n",
    "        }\n",
    "        \n",
    "        res = S.get(url=URL, params=PARAMS)\n",
    "        data = res.json()\n",
    "        wikitext = data['parse']['wikitext']['*']\n",
    "        lines = wikitext.split('|-')\n",
    "        paragraph = lines[0]\n",
    "        \n",
    "        #remove all unnecessary characters (defined in string b above)\n",
    "        for char in b:\n",
    "            paragraph = paragraph.replace(char, \"\")\n",
    "            \n",
    "        #if wordcount < 50, paragraph will be disregarded\n",
    "        if (len(paragraph.split(\" \")) < 50):\n",
    "            continue\n",
    "                \n",
    "        #removing words that do not carry any meaning (longer than 16 chars and smaller than 2)\n",
    "        for key in paragraph.split(\" \"):\n",
    "            if len(key) > 16:\n",
    "                paragraph = paragraph.replace(key, \"\")\n",
    "            \n",
    "        #line in this case is a whole paragraph\n",
    "        for sentence in paragraph.split(\".\"):\n",
    "            sentence = sentence.strip().split(\" \")\n",
    "            #disregard sentence if it has less than 10 words or starts with ref\n",
    "            if sentence[0] == 'ref' or len(sentence) < 10:\n",
    "                continue\n",
    "            #Stem each word separately\n",
    "            sentence = [stemmer.stem(i) for i in sentence]\n",
    "            #Check word by word, if the word is not in stop words and not a single letter, join\n",
    "            sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 2)\n",
    "            train.append(sentence)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabels(trainData, labelNumber):\n",
    "    #generating labels list that matches pageContent list in length\n",
    "    labels = []\n",
    "    for x in range(0, len(trainData)):\n",
    "        labels.append(labelNumber)\n",
    "    print(\"Labels made successfully\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def populateList(filename, keyword, limit=10000):\n",
    "    classList = []\n",
    "    notClassList = []\n",
    "    classCount = 0\n",
    "    notClassCount = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        #making a list of all lines from the file\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        #making a list of all lines that contain the keyword\n",
    "        for line in lines:\n",
    "            strippedLine = line.strip()\n",
    "            loweredLine = line.lower()\n",
    "            if keyword in loweredLine and classCount < limit:\n",
    "                classList.append(strippedLine)\n",
    "                classCount += 1\n",
    "        \n",
    "        #making a list of random lines of size(classList) that do not contain the keyword\n",
    "        while len(notClassList) != len(classList):\n",
    "            random_int = random.randint(0, len(lines)-1)\n",
    "            line = lines[random_int]\n",
    "            if keyword not in line.lower() and notClassCount < limit:\n",
    "                notClassList.append(line.strip())\n",
    "                notClassCount += 1\n",
    "    print(\"Article names in the class list: %d\" % len(classList))\n",
    "    print(\"Article names in the not class list: %d\" % len(notClassList))\n",
    "    return classList, notClassList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "securityList, notSecurityList = populateList(\"enwiki-latest-all-titles-in-ns0\", \"security\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102d_Security_Forces_Squadron  parsed successfully\n",
      "11th_Public_Security_Division_(People's_Republic_of_China)  parsed successfully\n",
      "12th_Public_Security_Division_(People's_Republic_of_China)  parsed successfully\n",
      "1966_United_Nations_Security_Council_election  parsed successfully\n",
      "1967_United_Nations_Security_Council_election  parsed successfully\n",
      "1968_United_Nations_Security_Council_election  parsed successfully\n",
      "1969_United_Nations_Security_Council_election  parsed successfully\n",
      "Mozaffarabad,_Bardaskan  parsed successfully\n",
      "Acacia_tetanophylla  parsed successfully\n",
      "Gayle_Ruzicka  parsed successfully\n",
      "\n",
      "Security length:  69\n",
      "Not Security length:  24\n",
      "!!!TEST IS INVALID!!!\n",
      "!!!TEST IS INVALID!!!\n"
     ]
    }
   ],
   "source": [
    "#Parse two wikipedia lists\n",
    "security = parseList(*securityList)\n",
    "notSecurity = parseList(*notSecurityList)\n",
    "print \"\\nSecurity length: \", len(security)\n",
    "print \"Not Security length: \", len(notSecurity)\n",
    "\n",
    "#Make entries be same length, pick minimum of two lengths\n",
    "deleteAfter = min(len(security), len(notSecurity))\n",
    "\n",
    "#If min length is less than 100 sentences, test is invalid\n",
    "if deleteAfter < 100:\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    del security[deleteAfter:]\n",
    "    del notSecurity[deleteAfter:]\n",
    "\n",
    "if(len(security) != len(notSecurity) and (len(security) != len(set(security)) or len(notSecurity) != len(set(notSecurity)))):\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    #Generate labels\n",
    "    trainLabels = makeLabels(security, 1)\n",
    "    trainLabels += makeLabels(notSecurity, 0)\n",
    "\n",
    "    #Collapse into single list\n",
    "    trainData = security + notSecurity\n",
    "\n",
    "    #Shuffle two lists, save the order\n",
    "    trainData, trainLabels = shuffle(trainData, trainLabels)\n",
    "    used, testLabels = shuffle(used, testLabels)\n",
    "\n",
    "    #Get validation data\n",
    "    validationToTrainRatio = 0.2\n",
    "    validationSize = int(validationToTrainRatio * len(trainData))\n",
    "    validationData = trainData[:validationSize]\n",
    "    validationLabels = trainLabels[:validationSize]\n",
    "    trainData = trainData[validationSize:]\n",
    "    trainLabels = trainLabels[validationSize:]\n",
    "\n",
    "    \n",
    "    #Sanity check\n",
    "    print \"\\nTest set length: \", len(used)\n",
    "    print \"Test labels set length: \", len(testLabels)\n",
    "    print \"\\nValidation set length: \", len(validationData)\n",
    "    print \"Validation labels set length: \", len(validationLabels)\n",
    "    print \"\\nTrain set length: \", len(trainData)\n",
    "    print \"Train labels set length: \", len(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts, test_texts):\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1,3),  # Use 1-grams + 2-grams + 3-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'min_df': 2, #Words that appear less than this value do not contribute\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    # Vectorize test texts.\n",
    "    x_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(20000, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9c168e365c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainData' is not defined"
     ]
    }
   ],
   "source": [
    "trainData, valData, testData = ngram_vectorize(trainData, trainLabels, validationData, used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the model method\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(keras.layers.Dense(units=units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=num_classes, activation=tf.nn.sigmoid))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For parameters refer to the upper cell\n",
    "model = mlp_model(2, 32, 0.3, trainData.shape[1:], 1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate could be further decreased for additional accuracy\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#callbacks will prevent model from running if val_loss starts to increase\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = model.fit(trainData,\n",
    "                    trainLabels,\n",
    "                    epochs=100,\n",
    "                    callbacks = callbacks,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(valData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalueating model on the testset\n",
    "#[loss, accuracy]\n",
    "print(model.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest of the notebook helps vidualize losses and accuracies by ploting two separate grophs\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf() #clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
