{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partially adopted from Tensorflow/docs/basic_text_classification\n",
    "#https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\n",
    "\n",
    "#As well as https://developers.google.com/machine-learning/guides/text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ParseList import parseList\n",
    "from GetSeeAlso import getSeeAlso\n",
    "from MakeLabels import makeLabels\n",
    "from PopulateList import populateList\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING SENTENCES TO LIBRARY\n",
    "import json\n",
    "\n",
    "#declare testLabels as an empty list\n",
    "testLabels = []\n",
    "\n",
    "#Keeping track of used sentences\n",
    "used = []\n",
    "\n",
    "#Keeping track of used sentences separately\n",
    "security = []\n",
    "nosecurity = []\n",
    "\n",
    "#this is the smallest number of unique non-functional requirements of a single class\n",
    "LIMIT = 270\n",
    "\n",
    "with open(\"Data/Consolidated_data.txt\",\"r\") as f:\n",
    "    \n",
    "    #Go line by line in original data file\n",
    "    for line in f:\n",
    "        #find where class begins\n",
    "        front = line.find(\"\\\"class\\\":\\\"\")\n",
    "        #find where class ends (where sentence begins)\n",
    "        end = line.find(\"\\\",\\\"sentence\\\":\\\"\")\n",
    "        #substring line based on front and end above\n",
    "        reqClass = (line[(front+9):end]).lower()\n",
    "\n",
    "        #sentence\n",
    "        temp = line.find(\"\\\"sentence\\\":\\\"\")\n",
    "        #Cut out the sentence part\n",
    "        sentence = (line[(temp+12):-4]).lower()\n",
    "        #Remove all symbols, numbers, and spaces. Split into list of words\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        #Unicode each word because stemmer likes it that way\n",
    "        sentence = [unicode(i, 'utf-8') for i in sentence]\n",
    "        #Stem each word separately\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        #Check word by word, if the word is not in stop words and not a single letter, join\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1)\n",
    "        \n",
    "        if sentence in used:\n",
    "            continue\n",
    "        else:\n",
    "            if \"security\" in reqClass and (len(security) < LIMIT):\n",
    "                used.append(sentence)\n",
    "                security.append(sentence)\n",
    "                testLabels.append(1)\n",
    "#             if \"security\" not in reqClass and (len(nosecurity) < LIMIT):\n",
    "#                 used.append(sentence)\n",
    "#                 nosecurity.append(sentence)\n",
    "#                 testLabels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "0\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "#Must all be balanced for valid results.\n",
    "#Commented out the total number of sentences in the document.\n",
    "print len(security) #270\n",
    "print len(nosecurity)\n",
    "print len(used) #540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "x_xl1 = []\n",
    "x_xl2 = []\n",
    "with open('Data/NFR.csv') as f:\n",
    "    for sentence in f:\n",
    "        cl = sentence[:sentence.find(',')]\n",
    "        sentence = sentence[sentence.find(',')+1:]\n",
    "        sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip().split(\" \")\n",
    "        sentence = [unicode(i, 'utf-8', errors='ignore') for i in sentence]\n",
    "        sentence = [stemmer.stem(i) for i in sentence]\n",
    "        sentence = ' '.join(word for word in sentence if word not in stopWords and len(word) > 1).strip()\n",
    "        if cl == 'Security':\n",
    "            x_xl2.append(sentence)\n",
    "        else:\n",
    "            x_xl1.append(sentence)\n",
    "del x_xl1[min(len(x_xl1), len(x_xl2)):]\n",
    "del x_xl2[min(len(x_xl1), len(x_xl2)):]\n",
    "\n",
    "print(len(x_xl1))\n",
    "print(len(x_xl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article names in the security list: 4000\n",
      "Article names in the not security list: 4000\n"
     ]
    }
   ],
   "source": [
    "securityList, notSecurityList = populateList(\"Data/enwiki-latest-all-titles-in-ns0\", \"security\", 4000, True)\n",
    "# seeAlso = getSeeAlso(*securityList)\n",
    "# securityList += seeAlso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels made successfully\n",
      "Labels made successfully\n"
     ]
    }
   ],
   "source": [
    "y_xl = makeLabels(x_xl1, 0)\n",
    "y_xl += makeLabels(x_xl2, 1)\n",
    "x_xl = x_xl1 + x_xl2\n",
    "x_xl, y_xl = shuffle(x_xl, y_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ldeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', BadStatusLine(\"''\",))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aeb4d76765df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Parse two wikipedia lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msecurity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msecurityList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnotSecurity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnotSecurityList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\nSecurity length: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecurity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Not Security length: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotSecurity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldeng/TL Requirements Classification/ParseList.pyc\u001b[0m in \u001b[0;36mparseList\u001b[0;34m(*titleList)\u001b[0m\n\u001b[1;32m     22\u001b[0m         }\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#if page did not parse successfully, continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldeng/.local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldeng/.local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldeng/.local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ldeng/.local/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', BadStatusLine(\"''\",))"
     ]
    }
   ],
   "source": [
    "#Parse two wikipedia lists\n",
    "security = parseList(*securityList)\n",
    "notSecurity = parseList(*notSecurityList)\n",
    "print \"\\nSecurity length: \", len(security)\n",
    "print \"Not Security length: \", len(notSecurity)\n",
    "\n",
    "#Make entries be same length, pick minimum of two lengths\n",
    "deleteAfter = min(len(security), len(notSecurity))\n",
    "\n",
    "#If min length is less than 100 sentences, test is invalid\n",
    "if deleteAfter < 100:\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    del security[deleteAfter:]\n",
    "    del notSecurity[deleteAfter:]\n",
    "\n",
    "if(len(security) != len(notSecurity) and (len(security) != len(set(security)) or len(notSecurity) != len(set(notSecurity)))):\n",
    "    print(\"!!!TEST IS INVALID!!!\")\n",
    "else:\n",
    "    #Generate labels\n",
    "    trainLabels = makeLabels(security, 1)\n",
    "    trainLabels += makeLabels(notSecurity, 0)\n",
    "\n",
    "    #Collapse into single list\n",
    "    trainData = security + notSecurity\n",
    "\n",
    "    #Shuffle two lists, save the order\n",
    "    trainData, trainLabels = shuffle(trainData, trainLabels)\n",
    "    #used, testLabels = shuffle(used, testLabels)\n",
    "\n",
    "    #Get validation data\n",
    "    validationToTrainRatio = 0.05\n",
    "    validationSize = int(validationToTrainRatio * len(trainData))\n",
    "    validationData = trainData[:validationSize]\n",
    "    validationLabels = trainLabels[:validationSize]\n",
    "    trainData = trainData[validationSize:]\n",
    "    trainLabels = trainLabels[validationSize:]\n",
    "\n",
    "    \n",
    "    #Sanity check\n",
    "    print \"\\nTest set length: \", len(used)\n",
    "    print \"Test labels set length: \", len(testLabels)\n",
    "    print \"\\nValidation set length: \", len(validationData)\n",
    "    print \"Validation labels set length: \", len(validationLabels)\n",
    "    print \"\\nTrain set length: \", len(trainData)\n",
    "    print \"Train labels set length: \", len(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts, test_texts):\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1,1),  # Use 1-grams + 2-grams + 3-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'min_df': 2, #Words that appear less than this value do not contribute\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    # Vectorize test texts.\n",
    "    x_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(20000, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainData, valData, testData = ngram_vectorize(trainData, trainLabels, validationData, x_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the model method\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(keras.layers.Dense(units=units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=32, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(keras.layers.Dense(units=num_classes, activation=tf.nn.sigmoid))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For parameters refer to the upper cell\n",
    "model = mlp_model(2, 64, 0.3, trainData.shape[1:], 1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate could be further decreased for additional accuracy\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#callbacks will prevent model from running if val_loss starts to increase\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = model.fit(trainData,\n",
    "                    trainLabels,\n",
    "                    epochs=20,\n",
    "                    callbacks = callbacks,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(valData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(testData)\n",
    "pred_labels = []\n",
    "occurences = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "for x in results:\n",
    "    if x <= 0.05:\n",
    "        occurences[0] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.15 and x > 0.05:\n",
    "        occurences[1] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.25 and x > 0.15:\n",
    "        occurences[2] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.35 and x > 0.25:\n",
    "        occurences[3] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.45 and x > 0.35:\n",
    "        occurences[4] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.55 and x > 0.45:\n",
    "        occurences[5] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.65 and x > 0.55:\n",
    "        occurences[6] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.75 and x > 0.65:\n",
    "        occurences[7] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.85 and x > 0.75:\n",
    "        occurences[8] += 1\n",
    "        pred_labels.append(0)\n",
    "    elif x <= 0.95 and x > 0.85:\n",
    "        occurences[9] += 1\n",
    "        pred_labels.append(1)\n",
    "    elif x <= 0.98 and x > 0.95:\n",
    "        occurences[10] += 1\n",
    "        pred_labels.append(1)\n",
    "    elif x > 0.98:\n",
    "        occurences[11] += 1\n",
    "        pred_labels.append(1)\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for x, y in zip(pred_labels, y_xl):\n",
    "    if x == 1 and y == 1:\n",
    "        TP += 1\n",
    "    elif x == 1 and y == 0:\n",
    "        FP += 1\n",
    "    elif x == 0 and y == 1:\n",
    "        FN += 1\n",
    "    elif x == 0 and y == 0:\n",
    "        TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print occurences\n",
    "accuracy = (TP+TN)/float(TP+TN+FP+FN)\n",
    "precision = (TP+TN)/float(TP+TN+FP)\n",
    "recall = (TP+TN)/float(TP+TN+FN)\n",
    "fscore = (2*precision*recall)/float(precision+recall)\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"\\nPrecision: %.4f\\nRecall: %.4f\\nF-score: %.4f\" % (precision, recall, fscore))\n",
    "print(\"\\nTP %d, TN %d, FP %d, FN %d\" % (TP, TN, FP, FN))\n",
    "\n",
    "df = pd.DataFrame({'Occurences': occurences}, index=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98])\n",
    "ax = df.plot.bar(rot=0)\n",
    "\n",
    "confusion_matrix(y_xl, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(y_xl, pred_labels)\n",
    "precision = precision_score(y_xl, pred_labels)\n",
    "recall = recall_score(y_xl, pred_labels)\n",
    "fscore = f1_score(y_xl, pred_labels)\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"\\nPrecision: %.4f\\nRecall: %.4f\\nF-score: %.4f\" % (precision, recall, fscore))\n",
    "\n",
    "confusion_matrix(y_xl, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
